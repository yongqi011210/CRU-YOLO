import math

import torch
from torch import nn
from ultralytics.nn.modules import Detect, Conv
from ultralytics.utils.tal import dist2bbox, dist2rbox, make_anchors

class RABR(nn.Module):
    """
    Rotation-Aware Bounding Box Regression (RABR)
    - è½»é‡åŒ–æ–¹å‘æ„ŸçŸ¥ç‰¹å¾æ¨¡å—
    - æ”¾åœ¨ OBB head å†…å¢å¼ºæ–¹å‘å»ºæ¨¡èƒ½åŠ›
    """
    def __init__(self, in_channels, ratio=0.25):
        super().__init__()
        mid_c = max(int(in_channels * ratio), 16)
        self.conv_angle = nn.Sequential(
            Conv(in_channels, mid_c, 3),
            Conv(mid_c, in_channels, 3)
        )
        # å¯å­¦ä¹ èåˆç³»æ•°
        self.alpha = nn.Parameter(torch.tensor(0.5))
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        feat_angle = self.conv_angle(x)
        w = self.sigmoid(self.alpha)
        return w * x + (1 - w) * feat_angle

class RABR_C(nn.Module):
    """
    Rotation-Aware Bounding Box Regression (RABR-C)
    - é€šé“æ³¨æ„åŠ›å¢å¼ºç‰ˆï¼šåœ¨RABR-SåŸºç¡€ä¸ŠåŠ å…¥SE-likeæ³¨æ„åŠ›
    - å…ˆåšæ–¹å‘å·ç§¯å¢å¼ºï¼Œå†é€šè¿‡é€šé“æ³¨æ„åŠ›è‡ªé€‚åº”åŠ æƒ
    """
    def __init__(self, in_channels, ratio=0.25, reduction=16):
        super().__init__()
        mid_c = max(int(in_channels * ratio), 16)

        # ğŸ”¹ æ–¹å‘å·ç§¯å¢å¼ºåˆ†æ”¯ï¼ˆåŒRABR-Sï¼‰
        self.conv_angle = nn.Sequential(
            Conv(in_channels, mid_c, 3),
            Conv(mid_c, in_channels, 3)
        )

        # ğŸ”¹ é€šé“æ³¨æ„åŠ›åˆ†æ”¯ï¼ˆSEç»“æ„ï¼‰
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction, in_channels, bias=False),
            nn.Sigmoid()
        )

        # ğŸ”¹ å¯å­¦ä¹ èåˆç³»æ•°
        self.alpha = nn.Parameter(torch.tensor(0.5))
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Step1: æ–¹å‘å·ç§¯å¢å¼º
        feat_angle = self.conv_angle(x)

        # Step2: é€šé“æ³¨æ„åŠ›
        b, c, _, _ = x.size()
        y = self.global_pool(feat_angle).view(b, c)
        attn = self.fc(y).view(b, c, 1, 1)  # é€šé“æƒé‡ [B,C,1,1]

        # Step3: é€šé“åŠ æƒèåˆ
        feat_attn = feat_angle * attn
        w = self.sigmoid(self.alpha)
        return w * x + (1 - w) * feat_attn

class RABR_M(nn.Module):
    """
    Multi-scale Rotation-Aware Feature Fusion (RABR-M)
    - å¯¹æ¯ä¸ªå°ºåº¦ï¼šå…ˆåšRABR_Sæ–¹å‘å¢å¼º
    - ä¸ä¸Šä¸€çº§(æ›´ç²—)åšä¸Šé‡‡æ ·èåˆã€ä¸ä¸‹ä¸€çº§(æ›´ç»†)åšä¸‹é‡‡æ ·èåˆ
    - concatåç”¨1Ã—1ConvæŠ•å½±å›åŸé€šé“
    """
    def __init__(self, channels: tuple):
        super().__init__()
        self.nl = len(channels)
        self.rabr = nn.ModuleList(RABR(c) for c in channels)

        self.up_map = nn.ModuleList()
        self.down_map = nn.ModuleList()
        self.merge = nn.ModuleList()

        for i in range(self.nl):
            # ä¸Šé‡‡æ ·æ˜ å°„ï¼šfrom i+1 â†’ i
            if i < self.nl - 1:
                self.up_map.append(Conv(channels[i + 1], channels[i], 1, 1))
            else:
                self.up_map.append(nn.Identity())

            # ä¸‹é‡‡æ ·æ˜ å°„ï¼šfrom i-1 â†’ i
            if i > 0:
                self.down_map.append(Conv(channels[i - 1], channels[i], 3, 2))
            else:
                self.down_map.append(nn.Identity())

            # èåˆå1Ã—1æŠ•å½±
            in_cat = channels[i]
            if i < self.nl - 1: in_cat += channels[i]
            if i > 0: in_cat += channels[i]
            self.merge.append(Conv(in_cat, channels[i], 1, 1))

        self.upsample = nn.Upsample(scale_factor=2, mode="nearest")

    def forward(self, feats):
        assert len(feats) == self.nl
        base = [self.rabr[i](feats[i]) for i in range(self.nl)]
        out = []

        for i in range(self.nl):
            parts = [base[i]]

            # ä¸Šé‡‡æ ·æ¥è‡ªæ›´ç²—å±‚
            if i < self.nl - 1:
                up = self.up_map[i](base[i + 1])
                up = self.upsample(up)
                if up.shape[-2:] != base[i].shape[-2:]:
                    up = nn.functional.interpolate(up, size=base[i].shape[-2:], mode="nearest")
                parts.append(up)

            # ä¸‹é‡‡æ ·æ¥è‡ªæ›´ç»†å±‚
            if i > 0:
                down = self.down_map[i](base[i - 1])
                if down.shape[-2:] != base[i].shape[-2:]:
                    down = nn.functional.interpolate(down, size=base[i].shape[-2:], mode="nearest")
                parts.append(down)

            fused = torch.cat(parts, 1)
            fused = self.merge[i](fused)
            out.append(fused)
        return out

class ChannelAttention(nn.Module):
    """SEç»“æ„ï¼šå…¨å±€å¹³å‡æ± åŒ– + FC + Sigmoid"""
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.shape
        y = self.avg_pool(x).view(b, c)
        w = self.fc(y).view(b, c, 1, 1)
        return x * w

class RABR_MC(nn.Module):
    """
    Multi-scale + Channel Attention Rotation-Aware Fusion (RABR-MC)
    - èåˆå¤šå°ºåº¦æ–¹å‘ä¸€è‡´æ€§å’Œé€šé“æ³¨æ„åŠ›
    - å¯¹æ¯ä¸ªå°ºåº¦ï¼šRABRå¢å¼º â†’ ä¸Šä¸‹å°ºåº¦èåˆ â†’ é€šé“åŠ æƒ â†’ æŠ•å½±
    """
    def __init__(self, channels: tuple, reduction=16):
        super().__init__()
        self.nl = len(channels)
        self.rabr = nn.ModuleList(RABR(c) for c in channels)
        self.ca = nn.ModuleList(ChannelAttention(c, reduction) for c in channels)

        self.up_map = nn.ModuleList()
        self.down_map = nn.ModuleList()
        self.merge = nn.ModuleList()
        self.upsample = nn.Upsample(scale_factor=2, mode="nearest")

        for i in range(self.nl):
            if i < self.nl - 1:
                self.up_map.append(Conv(channels[i + 1], channels[i], 1, 1))
            else:
                self.up_map.append(nn.Identity())

            if i > 0:
                self.down_map.append(Conv(channels[i - 1], channels[i], 3, 2))
            else:
                self.down_map.append(nn.Identity())

            in_cat = channels[i]
            if i < self.nl - 1:
                in_cat += channels[i]
            if i > 0:
                in_cat += channels[i]
            self.merge.append(Conv(in_cat, channels[i], 1, 1))

    def forward(self, feats):
        base = [self.rabr[i](feats[i]) for i in range(self.nl)]
        out = []

        for i in range(self.nl):
            parts = [base[i]]

            # ä¸Šé‡‡æ ·æ¥è‡ªæ›´ç²—å±‚
            if i < self.nl - 1:
                up = self.up_map[i](base[i + 1])
                up = self.upsample(up)
                if up.shape[-2:] != base[i].shape[-2:]:
                    up = nn.functional.interpolate(up, size=base[i].shape[-2:], mode="nearest")
                parts.append(up)

            # ä¸‹é‡‡æ ·æ¥è‡ªæ›´ç»†å±‚
            if i > 0:
                down = self.down_map[i](base[i - 1])
                if down.shape[-2:] != base[i].shape[-2:]:
                    down = nn.functional.interpolate(down, size=base[i].shape[-2:], mode="nearest")
                parts.append(down)

            fused = torch.cat(parts, dim=1)
            fused = self.merge[i](fused)

            # é€šé“æ³¨æ„åŠ›å†å¢å¼º
            fused = self.ca[i](fused)
            out.append(fused)
        return out



class CrossStageAttention(nn.Module):
    """Cross-Stage Attention (CSA)
    x:        å½“å‰é˜¶æ®µç‰¹å¾ [B, C, H, W]
    prev_feat:ä¸Šä¸€é˜¶æ®µåŒå°ºåº¦ç‰¹å¾ [B, C, H, W]
    """
    def __init__(self, channels, inter_channels=None):
        super().__init__()
        inter_channels = inter_channels or max(1, channels // 2)
        self.query_conv = nn.Conv2d(channels, inter_channels, 1)
        self.key_conv   = nn.Conv2d(channels, inter_channels, 1)
        self.value_conv = nn.Conv2d(channels, channels, 1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x, prev_feat):
        if prev_feat is None:
            return x

        # å¼ºçº¦æŸï¼šCSA ä¸¤ç«¯é€šé“å¿…é¡»ä¸€è‡´ï¼ˆç”±ä¸Šå±‚ç»“æ„ä¿è¯ï¼‰
        if prev_feat.shape[1] != x.shape[1]:
            raise RuntimeError(f"CSA channel mismatch: x={tuple(x.shape)}, prev={tuple(prev_feat.shape)}")

        q = self.query_conv(x)
        k = self.key_conv(prev_feat)
        v = self.value_conv(prev_feat)

        attn = torch.sigmoid(torch.mean(q * k, dim=1, keepdim=True))  # [B,1,H,W]
        return x + self.gamma.to(x.dtype) * (attn * v)

class CSAOrIdentity(nn.Module):
    """ç»Ÿä¸€ CSA/Identity æ¥å£ï¼šé¿å… ModuleList é‡Œæ”¾ Noneï¼Œä¹Ÿé¿å… Identity ä¸æ”¯æŒä¸¤ä¸ªå…¥å‚"""
    def __init__(self, channels: int, enabled: bool):
        super().__init__()
        self.enabled = bool(enabled)
        self.block = CrossStageAttention(channels) if self.enabled else nn.Identity()

    def forward(self, x, prev_feat=None):
        if (not self.enabled) or (prev_feat is None):
            return x
        return self.block(x, prev_feat)


# =========================
# RABR factory + wrapper
# =========================
class _PerLevelWrapper(nn.Module):
    """æŠŠå•è¾“å…¥æ¨¡å—æ‰©å±•åˆ°å¤šå°ºåº¦ list[feat] çš„ wrapper"""
    def __init__(self, channels, ctor):
        super().__init__()
        self.blocks = nn.ModuleList([ctor(c) for c in channels])

    def forward(self, feats):
        assert isinstance(feats, (list, tuple)), f"feats must be list/tuple, got {type(feats)}"
        assert len(feats) == len(self.blocks), f"len(feats)={len(feats)} != nl={len(self.blocks)}"
        return [blk(f) for blk, f in zip(self.blocks, feats)]


def build_rabr_block(channels, mode: str):
    """
    mode:
      - "none": ä¸ç”¨
      - "s":   RABR  (per-level)
      - "c":   RABR_C(per-level)
      - "m":   RABR_M
      - "mc":  RABR_MC
    """
    mode = (mode or "none").lower()
    if mode == "none":
        return nn.Identity()
    if mode == "s":
        return _PerLevelWrapper(channels, lambda c: RABR(c))
    if mode == "c":
        return _PerLevelWrapper(channels, lambda c: RABR_C(c))
    if mode == "m":
        return RABR_M(channels)
    if mode == "mc":
        return RABR_MC(channels)
    raise ValueError(f"Unknown rabr mode: {mode}")


# =========================
# OBB Cascade Head
# =========================
class OBB_CascadeHead(Detect):
    """
    Cascade ROI-based OBB Head (CSA + optional RABR)
    - è®­ç»ƒæ—¶å¯æŒ‡å®šå¯ç”¨/ç¦ç”¨ï¼šRABR / CSA / çº§è”é˜¶æ®µæ•°
    - âœ… ä¿®å¤ Ultralytics build-time stride æ¨æ–­ï¼šdummy forward åªè¿”å› det
    """
    def __init__(
        self,
        nc=80,
        ne=1,
        cascade_stages=2,
        use_csa=True,
        rabr_mode="mc",
        return_all_stages=False,
        debug=False,
        ch=(),  # âœ… æ”¾æœ€åï¼parse_model ä¼š append åˆ°æœ€å
    ):
        super().__init__(nc, ch)

        self.ne = int(ne)
        self.nl = len(ch)
        self.cascade_stages = int(cascade_stages)
        self.use_csa = bool(use_csa)
        self.return_all_stages = bool(return_all_stages)
        self.debug = bool(debug)

        # âœ… build-time stride inference gate
        # Ultralytics ä¼šåœ¨æ„å»ºé˜¶æ®µè·‘ä¸€æ¬¡ dummy forward æ¥æ¨ strideï¼Œ
        # è¿™æ—¶ head å¿…é¡»è¿”å› â€œå¯éå†çš„ tensor åˆ—è¡¨/å¼ é‡â€ï¼Œä¸èƒ½è¿”å› (det, angle) è¿™ç§åµŒå¥—ç»“æ„ã€‚
        self._stride_infer = True

        channels = tuple(ch)
        c4 = max(min(channels) // 4, 16, self.ne)

        # 1) Optional RABR
        self.rabr = build_rabr_block(channels, rabr_mode)

        # 2) Cascade branches
        self.trunks = nn.ModuleList()
        self.angle_preds = nn.ModuleList()
        self.angle_embeds = nn.ModuleList()

        for s in range(self.cascade_stages):
            trunk_s = nn.ModuleList()
            pred_s  = nn.ModuleList()
            emb_s   = nn.ModuleList()
            for xch in channels:
                trunk_s.append(nn.Sequential(
                    Conv(xch, c4, 3),
                    Conv(c4, c4, 3),
                ))
                pred_s.append(nn.Conv2d(c4, self.ne, 1))
                emb_s.append(nn.Conv2d(self.ne, xch, 1))
            self.trunks.append(trunk_s)
            self.angle_preds.append(pred_s)
            self.angle_embeds.append(emb_s)

        # 3) CSA blocks
        self.csa_blocks = nn.ModuleList()
        for s in range(self.cascade_stages):
            csa_s = nn.ModuleList()
            for xch in channels:
                csa_s.append(CSAOrIdentity(xch, enabled=(self.use_csa and s > 0)))
            self.csa_blocks.append(csa_s)

        print(f"[INFO] OBB_CascadeHead: stages={self.cascade_stages}, CSA={self.use_csa}, RABR={rabr_mode}")

    def forward(self, x):
        """
        x: list/tuple of multi-level features
        """
        bs = x[0].shape[0]

        # Step1: optional RABR
        feats = self.rabr(x)

        # Step2: cascade angle refinement (+ optional CSA)
        prev_embed = None
        all_angles = []

        for s in range(self.cascade_stages):
            angle_list = []
            curr_embed = []

            for i in range(self.nl):
                xi = feats[i]

                # CSA
                xi = self.csa_blocks[s][i](xi, None if prev_embed is None else prev_embed[i])

                trunk = self.trunks[s][i](xi)                 # [B, c4, H, W]
                angle_logits = self.angle_preds[s][i](trunk)  # [B, ne, H, W]

                angle_list.append(angle_logits.view(bs, self.ne, -1))

                emb = torch.tanh(self.angle_embeds[s][i](angle_logits))
                curr_embed.append(emb)

            angle_stage = torch.cat(angle_list, dim=2)
            angle_stage = (angle_stage.sigmoid() - 0.25) * math.pi
            all_angles.append(angle_stage)

            if self.debug:
                mn, mx = angle_stage.min().item(), angle_stage.max().item()
                print(f"[Stage {s}] angle range=({mn:.3f}, {mx:.3f})")

            prev_embed = curr_embed

        final_angle = all_angles[-1]
        if not self.training:
            self.angle = final_angle

        # Step3: YOLO detect output
        det = super().forward(feats)

        # =========================================================
        # âœ… Fix for Ultralytics build-time stride inference
        # During model building, Ultralytics runs a dummy forward to infer stride:
        #   m.stride = torch.tensor([s / x.shape[-2] for x in _forward(...)])
        # So _forward(...) must be iterable of Tensors. If we return (det, angle),
        # the first element is a list -> no .shape -> crash.
        #
        # Here we return ONLY det for the very first build-time forwards.
        # Once stride is set by framework, we disable this gate.
        # =========================================================
        if self._stride_infer:
            # stride ä¸€æ—¦è¢«æ¡†æ¶å†™å…¥ï¼ˆDetect/OBB head é€šå¸¸ä¼šæœ‰ stride å±æ€§ï¼‰ï¼Œå°±å…³é—­ gate
            if getattr(self, "stride", None) is not None:
                self._stride_infer = False
            return det

        # Step4: outputs (normal training/inference)
        if self.training:
            if self.return_all_stages:
                return det, final_angle, all_angles
            return det, final_angle

        # ---------- Inference / Export ----------
        if isinstance(det, torch.Tensor):
            return torch.cat([det, final_angle], 1)

        pred = det[0]
        aux  = det[1] if len(det) > 1 else None
        out_pred = torch.cat([pred, final_angle], 1)
        return (out_pred, (aux, final_angle))

    def decode_bboxes(self, bboxes, anchors):
        return dist2rbox(bboxes, self.angle, anchors, dim=1)
